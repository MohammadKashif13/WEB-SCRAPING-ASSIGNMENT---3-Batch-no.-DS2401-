{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb6b2b69",
   "metadata": {},
   "source": [
    "Question 1: Write a python program which searches all the product under a particular product from www.amazon.in. The \n",
    "product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for \n",
    "guitars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50c340c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\kasim\\anaconda3\\lib\\site-packages (4.18.1)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from selenium) (0.24.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from selenium) (2023.5.7)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from selenium) (4.10.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d31fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae72ec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_amazon(product):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(\"https://www.amazon.in\")\n",
    "    \n",
    "    search_box = driver.find_element_by_id(\"twotabsearchtextbox\")\n",
    "    search_box.send_keys(product)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    products = driver.find_elements_by_xpath('//span[@class=\"a-size-medium a-color-base a-text-normal\"]')\n",
    "    for product in products:\n",
    "        print(product.text)\n",
    "        \n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df01233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the product to search on Amazon.in: \")\n",
    "    search_amazon(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6f6da5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dbb6f51",
   "metadata": {},
   "source": [
    "Question 2: In the above question, now scrape the following details of each product listed in first 3 pages of your search \n",
    "results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then \n",
    "scrape all the products available under that product name. Details to be scraped are: \"Brand \n",
    "Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and \n",
    "“Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d1dac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\kasim\\anaconda3\\lib\\site-packages (4.18.1)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from selenium) (0.24.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from selenium) (2023.5.7)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from selenium) (4.10.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\kasim\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b96a750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63f7407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_product_details(product):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(\"https://www.amazon.in\")\n",
    "    \n",
    "    search_box = driver.find_element_by_id(\"twotabsearchtextbox\")\n",
    "    search_box.send_keys(product)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    brand_names = []\n",
    "    product_names = []\n",
    "    prices = []\n",
    "    return_exchange = []\n",
    "    expected_delivery = []\n",
    "    availabiliities = []\n",
    "    product_urls = []\n",
    "    \n",
    "    page_count = 0\n",
    "    while True:\n",
    "        product_containers = driver.find_elements_by_xpath('//div[@data-component-type=\"s-search-result\"]')\n",
    "        \n",
    "        for product in product_containers:\n",
    "            try:\n",
    "                brand_name = product.find_element_by_xpath('.//span[@class=\"a-size-base-plus a-color-base\"]').text\n",
    "            except:\n",
    "                brand_name = \"-\"\n",
    "                \n",
    "            try:\n",
    "                product_name = product.find_element_by_xpath('.//span[@class=\"a-size-medium a-color-base a-text-normal\"]').text\n",
    "            except:\n",
    "                product_name = \"-\"\n",
    "                \n",
    "            try:\n",
    "                price = product.find_element_by_xpath('.//span[@class=\"a-price-whole\"]').text\n",
    "            except:\n",
    "                price = \"-\"\n",
    "                \n",
    "            try:\n",
    "                return_exchange_info = product.find_element_by_xpath('.//span[contains(text(), \"Return or Replacement\")]/parent::spain').text\n",
    "            except:\n",
    "                return_exchange_info = \"-\"\n",
    "                \n",
    "            try:\n",
    "                delivery_info = product.find_element_by_xpath('.//span[contains(text(), \"FREE Delivery\") or contains(text(), \"Get it by\")]/parent::span').text\n",
    "            except:\n",
    "                delivery_info = \"-\"\n",
    "                \n",
    "            try:\n",
    "                availability = product.find_element_by_xpath('.//span[@class=\"a-size-base a-color-success\"]').text\n",
    "            except:\n",
    "                availability = \"-\"\n",
    "                \n",
    "            try:\n",
    "                product_url = product.find_element_by_xpath('.//a[@class=\"a-link-normal s-no-outline\"]').get_attribute('href')\n",
    "            except:\n",
    "                product_url = \"-\"\n",
    "                \n",
    "                \n",
    "            brand_names.append(brand_name)\n",
    "            product_names.append(product_name)\n",
    "            prices.append(price)\n",
    "            return_exchange.append(return_exchange_info)\n",
    "            expected_delivery.append(delivery_info)\n",
    "            availabilities.append(availability)\n",
    "            product_urls.append(product_url)\n",
    "            \n",
    "            \n",
    "        try:\n",
    "            next_page = driver.find_element_by_xpath('//li[@class=\"a-last\"]/a')\n",
    "            next_page.click()\n",
    "            page_count += 1\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            break\n",
    "            \n",
    "        if page_count >= 2:\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e520d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"Brand Name\": brand_names,\n",
    "    \"Name of the Product\": product_names,\n",
    "    \"Price\": prices,\n",
    "    \"Return/Exchange\": return_exchange,\n",
    "    \"Expected Delivery\": expected_delivery,\n",
    "    \"Availability\": availabilities,\n",
    "    \"Product Url\": product_urls\n",
    "})\n",
    "\n",
    "df.to.csv(\"amazon_products.csv\", index=False)\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68e4697",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the product to search on Amazon.in: \")\n",
    "    scrape_product_details(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe49a06e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cea8337",
   "metadata": {},
   "source": [
    "Question 3: Write a python program to access the search bar and search button on images.google.com and scrape 10 \n",
    "images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19bae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f2cf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38493f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_images(keywords, num_images=10):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(\"https://www.images.google.com\")\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        search_bar = driver.find_element_by_name(\"q\")\n",
    "        search_bar.clear()\n",
    "        search_bar.send_keys(keyword)\n",
    "        \n",
    "        search_button = driver.find_element_by_css_selector(\"button[type='submit']\")\n",
    "        search_button.click()\n",
    "        time.sleep(2)\n",
    "        \n",
    "        for _ in range(3):\n",
    "            driver.execute_script(\"window.scrollTo(O, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "            \n",
    "        image_elements = driver.find_elements_by_css_selector(\".rg_i.Q4LuWd\")\n",
    "        image_urls = []\n",
    "        for img in image_elements[:num_images]:\n",
    "            img.click()\n",
    "            time.sleep(2)\n",
    "            try:\n",
    "                orignal_image = driver.find_element_by_css_selector(\".n3VNCb\").get_attribute(\"src\")\n",
    "                image_urls.append(original_image)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        print(f\"Scraped {len(image_urls)} images for '{keyword}':\")\n",
    "        for url in image_urls:\n",
    "            print(url)\n",
    "        print()\n",
    "    \n",
    "    driver.quit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e9c774",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    keywords = ['fruits', 'car', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "    scrape_images(keywords, num_images=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35e6d64",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69904519",
   "metadata": {},
   "source": [
    "Question 4:Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com\n",
    "and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand \n",
    "Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, \n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the \n",
    "details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aed98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ead8570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787518b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_smartphone_details(product):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(\"https://www.flipkart.com\")\n",
    "    \n",
    "    try:\n",
    "        driver.find_element_by_xpath('//button[@class=\"_2KpZ61 _2doB4z\"]').click()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    search_bar = driver.find_element_by_xpath('//input[@title=\"Search for products, brands and more\"]')\n",
    "    search_bar.send_keys(product)\n",
    "    search_bar.submit()\n",
    "    \n",
    "    driver.implicitly_wait(5)\n",
    "    \n",
    "    details = []\n",
    "    product = driver.find_elements_by_xpath('//div[@class=\"_4rRO1T\"]')\n",
    "    for product in products:\n",
    "        details.append({\n",
    "            \"Brand Name\": product.text.split()[0],\n",
    "            \"Smartphone name\": product.text,\n",
    "            \"Colour\": \"-\",\n",
    "            \"RAM\": \"-\",\n",
    "            \"Storage(ROM)\": \"-\",\n",
    "            \"Primary Camera\": \"-\",\n",
    "            \"Secondary Camera\": \"-\",\n",
    "            \"Display Size\": \"-\",\n",
    "            \"Battery Capacity\": \"-\",\n",
    "            \"Price\": product.text.split()[1],\n",
    "            \"Product URL\": product.find_element_by_xpath('.//..').get_attribute('href')\n",
    "        })\n",
    "        \n",
    "    driver.quit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0137efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(details)\n",
    "df.to_csv(\"smartphone_details.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26adf93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    product = input(\"Enter the smartphone to search on Flipkart: \")\n",
    "    scrape_smartphone_details(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196418a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78906b76",
   "metadata": {},
   "source": [
    "Question 5:Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d484286",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d453852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee156db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_coordinates(city):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(f\"https://www.google.com/maps//place/{city}\")\n",
    "    coordinates = driver.current_url.split('@')[1].split(',')[0:2]\n",
    "    latitude, longitude = map(float, coordinates)\n",
    "    driver.quit()\n",
    "    return latitude, longitude\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    city = input(\"Enter the city to search on Google Maps: \")\n",
    "    latitude, longitude = scrape_coordinates(city)\n",
    "    print(\"Latitude:\", latitude)\n",
    "    print(\"Longitude:\", longitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762da538",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10bf0c0d",
   "metadata": {},
   "source": [
    "Question 6:Write a program to scrap all the available details of best gaming laptops from digit.in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c954c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d36de90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32439ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_gaming_laptops():\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(\"https://www.digit.in/top-products/best-gaming-laptops-40.html\")\n",
    "    \n",
    "    details = []\n",
    "    laptops = driver.find_element_by_xpath('//div[@class=\"TopNumbeHeading sticky-footer\"]')\n",
    "    for laptop in laptops:\n",
    "        details.append({\n",
    "            \"Name\": laptop.find_element_by_xpath('.//h3').text,\n",
    "            \"Processor\": laptop.find_element_by_xpath('.//p[contains(text(), \"Processor\")]/span').text,\n",
    "            \"RAM\": laptop.find_element_by_xpath('.//p[contains(text(), \"Memory\")]/span').text,\n",
    "            \"OS\": laptop.find_element_by_xpath('.//p[contains(text(), \"OS\")]/span').text,\n",
    "            \"Storage\": laptop.find_element_by_xpath('.//p[contains(text(), \"Storage\")]/span').text,\n",
    "            \"Display\": laptop.find_element_by_xpath('.//p[contains(text(), \"Display\")]/span').text,\n",
    "            \"Price\": laptop.find_element_by_xpath('.//b').text\n",
    "        })\n",
    "        \n",
    "    driver.quit()\n",
    "    \n",
    "    df = pd.DataFrame(details)\n",
    "    df.to_csv(\"gaming_laptops_details.csv\", index=false)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ed72ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    scrape_gaming_laptops()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462a98a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55353dba",
   "metadata": {},
   "source": [
    "Question 7:Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: \n",
    "“Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f8a4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c11fc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030215d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_billionaires():\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(\"https://www.forbes.com/billionaires/\")\n",
    "    \n",
    "    details = []\n",
    "    rows = driver.find_elements_by_xpath('//div[@class=\"rank\"]//div[@class=\"personName\"]')\n",
    "    for row in rows:\n",
    "        rank = row.find_element_by_xpath('.//..').text.split()[0]\n",
    "        name = row.find_element_by_xpath('.//..').text.split()[1:]\n",
    "        name = ' '.join(name)\n",
    "        net_worth = row.find_element_by_xpath('../../..//div[@class=\"netWorth\"]').text\n",
    "        age = row.find_element_by_xpath('../../..//div[@class=\"age\"]').text\n",
    "        citizenship = row.find_element_by_xpath('../../..//div[@class=\"countryofcitizenship\"]').text\n",
    "        source = row.find_element_by_xpath('../../..//div[@class=\"source\"]').text\n",
    "        industry = row.find_element_by_xpath('../../..//div[@class=\"category\"]').text\n",
    "        \n",
    "        details.append({\n",
    "            \"Rank\": rank,\n",
    "            \"Name\": name,\n",
    "            \"Net worth\": net_worth,\n",
    "            \"Age\": age,\n",
    "            \"Citizenship\": citizenship,\n",
    "            \"Source\": source,\n",
    "            \"Industry\": industry\n",
    "        })\n",
    "        \n",
    "    driver.quit()\n",
    "    \n",
    "    df = pd.DataFrame(details)\n",
    "    df.to_csv(\"billionaires_details.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eefb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    scrape_billionaires()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebf24cf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20e5713c",
   "metadata": {},
   "source": [
    "Question 8:Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted \n",
    "from any YouTube Video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ccb4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f13fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4110da21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_comments(video_url):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(video_url)\n",
    "    driver.execute_script('window.scrollTo(0, 10000)')\n",
    "    time.sleep(5)\n",
    "    \n",
    "    comments = driver.find_elements_by_xpath('//*[@id=\"content-text\"]')\n",
    "    upvotes = driver.find_elements_by_xpath('//*[@id=\"vote-count-middle\"]')\n",
    "    timestamps = driver.find_elements_by_xpath('//*[@class=\"published-time-text above-comment style-scrape ytd-comment-renderer\"]')\n",
    "    \n",
    "    comment_data = []\n",
    "    for comment, upvote, timestamp in zip(comments, upvote, timestamps):\n",
    "        comment_data.append({\n",
    "            \"Comment\": comment.text,\n",
    "            \"Upvotes\": upvote.text,\n",
    "            \"Timestamp\": timestamp.get_attribute(\"innerText\")\n",
    "        })\n",
    "        if len(comment_data) >= 500:\n",
    "            break\n",
    "            \n",
    "            \n",
    "    driver.quit()\n",
    "    return comment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817cfc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    video_url = input(\"Enter the url of the YouTube video: \")\n",
    "    comments = extract_comments(video_url)\n",
    "    print(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a847d44",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b503efa6",
   "metadata": {},
   "source": [
    "Question 9:Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in \n",
    "“London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall \n",
    "reviews, privates from price, dorms from price, facilities and property description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98feffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e096a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ebf903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_hostels_in_london():\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(\"https://www.hostelworld.com/s?q=London,%20England&country=England&city=London&type=city&id=3&from=2022-04-13&to=2022-04-14&guests=1&page=1\")\n",
    "    \n",
    "    \n",
    "    details = []\n",
    "    hostels = driver.find_element_by_xpath('//div[@class=\"fabreviewitem fabreviewitem-rows\"]')\n",
    "    for hostel in hostels:\n",
    "        name = hostel.find_element_by_xpath('.//h2/a').text\n",
    "        distance = hostel.find_element_by_xpath('.//div[@class=\"distanceFromText\"]').text\n",
    "        ratings = hotel.find_element_by_xpath('.//div[@class=\"score orange big\"]').text\n",
    "        total_reviews = hostel.find_element_by_xpath('.//din[@class=\"reviews\"]').text\n",
    "        overall_reviews = hostel.find_element_by_xpath('.//div[@class=\"keyword\"]').text\n",
    "        privates_prive = hostel.find_element_by_xpath('.//span[contains(text(), \"Privates From\")]/following-sibling::span').text\n",
    "        dorms_price = hostel.find_element_by_xpath('.//span[contains(text(), \"Dorms From\")]/following-sibling::span').text\n",
    "        facilities = [facility.text for facility in hostel.find_element_by_xpath('.//ul[@class=\"facilities\"]/li')]\n",
    "        description = hostel.find_element_by_xpath('.//div[@class=\"rating-factors prop-card-tablet-style-cardTablet__ratingfactors__1k4jB\"]').text\n",
    "        \n",
    "        details.append({\n",
    "            \"Hostel Name\": name,\n",
    "            \"Distance from City Centre\": distance,\n",
    "            \"Rating\": ratings,\n",
    "            \"Total Reviews\": total_reviews,\n",
    "            \"Overall Reviews\": overall_reviews,\n",
    "            \"Privates Price\": privates_price,\n",
    "            \"Dorms Price\": dorms_price,\n",
    "            \"Property Description\": description\n",
    "        })\n",
    "        \n",
    "    driver.quit()\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(details)\n",
    "    df.to.csv(\"hostel_in_london.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c9b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    scrape_hostels_in_london()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50151c24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914061cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dc376e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d40147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8dde01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
